# Integrating Non-Parametric Attention and Prompt Refinement to Enhance LLM-BasedText-to-SQL Without External Knowledge
Generate, execute, and iteratively refine _valid_ SQLite queries from naturalâ€‘language questions using OpenAI chat models.

---

## At a Glance
| Problem | This projectâ€™s answer |
|---------|-----------------------|
| Large schema, irrelevant tables â†’ hallucinated joins | **Entityâ€‘aware *soft masking*** â€“ numeric weights hint the model toward relevant tables/columns. |
| Unreliable SQL from LLM | **Execution + feedback loop** â€“ run the query, analyse results, regenerate up to 3Ã—. |
| Slow schema access | **Onâ€‘disk cache** (`./schema_cache/*.json`). |
| Need auditability | Full attempt history with validation feedback in `feedback_<mode>.json`. |

---

## Framework Overview

![Framework Overview](./images/attention_pipeline.png)


This framework improves LLM-based Text-to-SQL generation using **non-parametric attention** and **prompt refinement**, without external knowledge.

---

### ðŸ” A. Data Preprocessing

- Extracts named entities and relations using **SpaCy** (`en_core_web_sm`).
- Identified entities are mapped to relevant schema elements.

---

### ðŸŽ¯ B. Non-Parametric Attention

Assigns soft relevance weights to tables/columns based on entity matches:

| Type     | Matched Weight       | Unmatched Weight |
|----------|----------------------|------------------|
| Tables   | `1.0 + hits`         | `0.5`            |
| Columns  | `1.0 + hits`         | `0.3`            |

- Relation-aware scores update weights.
- Final weights are max-normalized.
- Weights appear in prompts via:
  - Inline SQL comments
  - Schema summary blocks

---

### âœï¸ C. Prompt Refinement

- Prompts include:
  - NL query
  - Weighted schema
  - SQL generation instruction
- After SQL generation:
  - Query is executed and scored:
    - Success (0.5), row range reward/penalty, error penalty
  - If confidence score (`CS`) is low, regenerate with:
    - Feedback
    - Previous attempt summary

---

---

##  Attention / Masking Mechanism

1. **Entity & dependency mining**  
   SpaCy detects named entities _and_ subject/â€‹object pairs in the question.

2. **Schema mapping â†’ weights**  
   *â€¯+1.0â€¯* for exact hits on table/column names.  
   *â€¯+0.5â€¯* extra if either side of a dependency pair appears.  
   Nonâ€‘relevant elements receive a floor weight of **0.3**.

3. **Soft mask, not hard filter**  
   The weights are **exposed inside the prompt** in two ways:

   *Inline comments*â€¯inside each `CREATE TABLE â€¦`:

   ```sql
   CREATE TABLE orders (
       order_id INTEGER PRIMARY KEY,          -- AttentionÂ Weight: 1.5
       order_date TEXT                        -- AttentionÂ Weight: 1.5
   ) -- AttentionÂ Weight: 1.5
   ```

   *Standalone block*:

   ```
   ### Schema Attention Weights ###
   orders: 1.5
   orders.order_date: 1.5
   customers: 0.3
   ...
   ```

   > The LLM remains free to use any part of the schema, but higherâ€‘weighted
   > elements are statistically more likely to be selected â€“ a **soft mask**.

---

## âœ¨  Feature Table
| Area | Details |
|------|---------|
| **Schema cache** | JSON snapshot of DDL + columns for each DB (`SchemaCache`). |
| **Fewâ€‘shot** | Oneâ€‘shot demo (with or without external knowledge). |
| **Chainâ€‘ofâ€‘Thought (optional)** | `--chain_of_thought True` injects a CoT instruction but strips CoT from final answer. |
| **Selfâ€‘validation loop** | Up to 3 cycles: execute â†’ analyse â†’ feedback â†’ regenerate. |
| **Confidence scoring** | Combines execution success, row count, and issue count. |
| **Timeouts & retries** | `signal.alarm` for code/DB, `backoff` for rateâ€‘limits. |

---

## Installation

```bash
python -m venv venv
source venv/bin/activate  # Windows: .env\Scriptsctivate
pip install -U openai backoff sqlparse tqdm spacy
python -m spacy download en_core_web_sm
```

> `signal.alarm` requires POSIX (Linux/macOS). On Windows use WSL or adapt to `multiprocessing`.

---

##  Data Layout

```
project/dev/
â”œâ”€â”€ databases/
â”‚   â””â”€â”€ <db_id>/<db_id>.sqlite
â””â”€â”€ eval/
    â””â”€â”€ dev.json
```

`dev.json` object schema:

```json
{
  "question": "How many orders were shipped to Canada in 2024?",
  "db_id":    "northwind",
  "evidence": "Canada appears in the 'Customers' table under 'Country'."
}
```
Please download the BIRD dataset from- https://bird-bench.github.io/
---

## Usage

```bash
export OPENAI_API_KEY="sk-..."   # or pass via --api_key

python nl2sql_attention.py   --eval_path   eval/eval.json   --mode  dev   --db_root_path       databases   --api_key            $OPENAI_API_KEY   --engine    gpt-4o   --data_output_path   outputs/   --feedback_output_path outputs/feedback_dev.json   --use_knowledge      False   --chain_of_thought   False
```

Outputs:

```
outputs/
â”œâ”€â”€ predict_dev.json   # {int: SQL string}
â””â”€â”€ feedback_dev.json  # perâ€‘question attempt log
```

---

## âš ï¸  Limitations 

* `--use_knowledge` flag is parsed but not threaded into prompt builder since we focus on the text2sql case without usage of any external knowledge.
* For the evaluation, we use EX and VES defined by the original BIRD repository (https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird)

---

> Found a bug or have an improvement? PRs welcome!
